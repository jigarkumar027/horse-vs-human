{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhXL7tM91Rai",
        "colab_type": "text"
      },
      "source": [
        "to get started with this exiting project first of all download the dataset from this given link.\n",
        "\n",
        "here you will get the zip file of the dataset which will extracted into the two seperate folder with name as horse and human. And this is same for both zip file.\n",
        "\n",
        "one zip  file for training data and another one for validation(test) data.\n",
        "\n",
        "**training dataset** : https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "**test dataset**: https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip\n",
        "\n",
        "\n",
        "\n",
        "now import the os and zipfile module to get your file path and to extract the zipfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4tmuA1Vge8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip='your full path to training zip file'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('name that extracted file')\n",
        "local_zip = 'fulll path to test zip file'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('name that extracted file')\n",
        "zip_ref.close()\n",
        "\n",
        "train_horse_dir = os.path.join('folder name and path of that extracted file (/temp/horse-or-human/horse)')\n",
        "train_human_dir = os.path.join('folder name and path of that extracted file(/temp/horse-or-human/humans)')\n",
        "validation_horse_dir = os.path.join('folder name and path of that extracted file(/tmp/validation-horse-or-human/horses)')\n",
        "validation_human_dir = os.path.join('folder name and path of that extracted file(/tmp/validation-horse-or-human/humans)')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciohzqDh51iN",
        "colab_type": "text"
      },
      "source": [
        "import the tensorflow library to get started with our nural network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FThKjGbflsqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtfloW0G6HDK",
        "colab_type": "text"
      },
      "source": [
        "here we are building our nural network.\n",
        "\n",
        "**first layer**: it is a convolution layer with a filter of 3*3  matrix with total of 16 windows.\n",
        "The activation function we used is a relu rectification function .\n",
        "the inputshape is length of input image means 300*300 with a 3 channel of RGB.\n",
        "polling layer has a 2*2 matrix\n",
        "\n",
        "**second layer**:  same as first layer except the input_shape not given as its a not a first layer.\n",
        "\n",
        "same for **third**, **fourth**and **fifth** layer .\n",
        "\n",
        "now after that we have our deep nural network.\n",
        " \n",
        "The main advantage of CNN is that it gets the main feature of the input images so that our DNN does not have to work for whole image instade only have to focus on the some feature which givves us high accuracy.\n",
        "\n",
        "At last there is a output layer with a activation finction as a sigmod function.\n",
        "we used a  sigmoid function  because there is only two posiibility that horse or human.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kYQixdVl0eP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    # DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv2TILJZ-IPz",
        "colab_type": "text"
      },
      "source": [
        "Here we defined a loss function as binary_crossentropy due to binay classification.\n",
        "optimizer we used is RMSprop.\n",
        "matrics is for how we want to evaluate our  network .\n",
        "here with accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN1S-tg-m6eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=.0001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRw4YhYW_NON",
        "colab_type": "text"
      },
      "source": [
        "The package we imported here is image generator.\n",
        "The image generator is wonderfull package to play with a image in tensorflow.\n",
        "it gives a easy interaction witha a images where we can modified images dimentions with different aurgument available. \n",
        "the resone for rescale is to make it in the range of 0 to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYLHWZBvnD64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255 )\n",
        "\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized \n",
        "        batch_size=32,\n",
        "        class_mode='binary'     # Since we use binary_crossentropy loss, we need binary labels\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mquoBR7tBO1l",
        "colab_type": "text"
      },
      "source": [
        "The next stape is to fit our network.\n",
        "here we defined the total no. of epoch to iterate with no. of batches and with diffrent argument of train and validation generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtXYZjBBn3yM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YYvVyfNCD5T",
        "colab_type": "text"
      },
      "source": [
        "now the next stape is to plote our accuracy and the loss chart to visulize our nural networks efficiency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aO5fEfMoCYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FLtOx_0CzNo",
        "colab_type": "text"
      },
      "source": [
        "as network trained per epoch we have to  see the level of accuracy and loss se that we can eveluate that wether our model is getting overfit or underfit.\n",
        " and if our model is giving the 100% accuracy on trained data then its a possibility of overfiting and then we have to modify some perameter. \n",
        " we can give more data with the use of  augmentation techniques.\n",
        " and we could use a dropout. \n",
        "\n",
        " there is currently lots of techniques to overcome from overfitting like regularization."
      ]
    }
  ]
}